{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformer \n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"[Tut](https://www.youtube.com/watch?v=SMZQrJ_L1vo&pp=ygUmbWFjaGluZSBsZWFybmluZyB0cmFuc2Zvcm1lciBleHBsYWluZWQ%3D)","metadata":{}},{"cell_type":"markdown","source":"[](http://)","metadata":{}},{"cell_type":"markdown","source":"## What it does (basic)","metadata":{}},{"cell_type":"markdown","source":"* The Transformer is a type of neural network architecture specifically designed for sequence-to-sequence tasks, such as machine translation, text generation, and language understanding.\n* It was introduced in a paper titled \"Attention is All You Need\" by Vaswani et al. in 2017 and has since become a fundamental building block in many natural language processing (NLP) tasks.\n* The key innovation in the Transformer is the attention mechanism, which allows the model to focus on different parts of the input sequence when generating the output.\n* Unlike traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs), the Transformer does not rely on sequential processing or fixed-size convolutions, making it highly parallelizable and more efficient for longer sequences.\n* The Transformer consists of an encoder and a decoder. The encoder processes the input sequence and extracts its contextual representation, while the decoder generates the output sequence based on that representation.\n* Both the encoder and decoder are composed of multiple layers of self-attention mechanisms and feed-forward neural networks.\n* Self-attention allows the model to weigh the importance of different positions in the input sequence, capturing long-range dependencies and improving performance on tasks requiring understanding of global context.\n* The attention mechanism computes attention weights by comparing each position in the sequence with every other position, capturing both local and global information.\n* The feed-forward neural networks within each layer help to transform and refine the representations learned by the self-attention mechanism.\n* Transformers have achieved state-of-the-art results in various NLP tasks, including machine translation, question answering, sentiment analysis, and text summarization.\n* Popular implementations of the Transformer architecture include the models known as \"BERT\" (Bidirectional Encoder Representations from Transformers) and \"GPT\" (Generative Pre-trained Transformer), which have significantly advanced the field of NLP.","metadata":{}},{"cell_type":"markdown","source":"## Explain it to a six year old","metadata":{}},{"cell_type":"markdown","source":"* The Transformer is a special computer program that helps us understand and talk with computers using words.\n* It can translate words from one language to another, like magic!\n* It has a superpower called \"attention\" that helps it focus on important parts of the words.\n* It doesn't read words one by one like we do, but can understand the meaning of a whole sentence at once.\n* The Transformer has two parts: an \"encoder\" that learns about the words we give it, and a \"decoder\" that helps it give us the answers or translations we want.\n* The Transformer is really good at understanding and making sense of what we say, even if the words are complicated or the sentence is long.\n* Many smart computers and apps use the Transformer to help us talk to them and get better answers.","metadata":{}},{"cell_type":"markdown","source":"## Mathematically ","metadata":{}},{"cell_type":"markdown","source":"1. Self-Attention Mechanism:\n\n* The self-attention mechanism computes attention weights for each word in a sequence based on its relationship with other words.\n* Given an input sequence of words X = {x₁, x₂, ..., xₙ}, the self-attention mechanism calculates the attention weights using three learned matrices: Query (Q), Key (K), and Value (V).\n* The attention weights are computed as follows:\n* Query matrix: Q = X * WQ\n* Key matrix: K = X * WK\n* Value matrix: V = X * WV\n* Attention weights: A = softmax(QKᵀ / √d) (element-wise division by the square root of the dimension d)\n* Here, Q, K, and V are matrices, and WQ, WK, and WV are learned weight matrices.\n\n2. Contextual Representation:\n\n* The self-attention mechanism uses the attention weights to compute a weighted sum of the Value matrix to obtain the contextual representation for each word.\n* Contextual representation: C = A * V\n\n3. Transformer Encoder:\n\n* The Transformer encoder consists of multiple layers of self-attention and feed-forward neural networks.\n* The output of one layer serves as the input to the next layer.\n* The self-attention mechanism is applied to the input sequence, and the resulting contextual representation is then passed through a feed-forward neural network.\n* The feed-forward neural network applies two linear transformations followed by a non-linear activation function like ReLU.\n* The output of the feed-forward network is added to the input sequence to obtain the final output of the encoder layer.\n\n4. Transformer Decoder:\n\n* The Transformer decoder also consists of multiple layers of self-attention and feed-forward neural networks, similar to the encoder.\n* In addition to the self-attention mechanism, the decoder also uses an additional attention mechanism to focus on the encoder's output.\n* This encoder-decoder attention mechanism helps the decoder to understand the context from the encoder's input.\n* The decoder takes as input the previous words in the output sequence and generates the next word using a similar process as the encoder, but with an additional attention mechanism.","metadata":{}},{"cell_type":"markdown","source":"## Libraries ","metadata":{}},{"cell_type":"markdown","source":"Transformers (Hugging Face's library for pre-trained models and fine-tuning):\nscikit-learn:\n\n","metadata":{}},{"cell_type":"code","source":"import transformers as tfms\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PyTorch (Deep learning framework):\n","metadata":{}},{"cell_type":"code","source":"import torch\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TensorFlow (Deep learning framework):\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Keras (High-level neural networks API, works with TensorFlow):\n","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras as keras\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BERT (Pre-trained Transformer model for language understanding):\n","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel, BertTokenizer\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GPT (Pre-trained Transformer model for language generation):\n","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2Model, GPT2Tokenizer\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RoBERTa (Robustly optimized BERT model):\n","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaModel, RobertaTokenizer\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"T5 (Text-to-Text Transfer Transformer):\n","metadata":{}},{"cell_type":"code","source":"from transformers import T5Model, T5Tokenizer\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XLNet (Pre-trained model based on Transformer-XL):\n","metadata":{}},{"cell_type":"code","source":"from transformers import XLNetModel, XLNetTokenizer\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DistilBERT (Lightweight version of BERT):\n","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertModel, DistilBertTokenizer\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"markdown","source":"Tokenization using Transformers library:","metadata":{}},{"cell_type":"code","source":"import transformers as tfms\n\n# Load tokenizer\ntokenizer = tfms.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Tokenize input text\ntext = \"Hello, how are you?\"\ntokens = tokenizer.tokenize(text)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading pre-trained Transformer models:","metadata":{}},{"cell_type":"code","source":"import transformers as tfms\n\n# Load pre-trained model\nmodel = tfms.AutoModel.from_pretrained(\"bert-base-uncased\")\n\n# Generate model output\ninput_ids = [1, 2, 3, 4]  # Example input\noutputs = model(input_ids)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fine-tuning Transformers models with PyTorch:","metadata":{}},{"cell_type":"code","source":"import transformers as tfms\nimport torch\n\n# Define model architecture\nmodel = tfms.AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Training loop\nfor epoch in range(num_epochs):\n    # Forward pass\n    outputs = model(input_ids)\n    logits = outputs.logits\n\n    # Calculate loss\n    loss = loss_fn(logits, labels)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generating text using Transformers models:","metadata":{}},{"cell_type":"code","source":"import transformers as tfms\n\n# Load model and tokenizer\nmodel = tfms.AutoModelForCausalLM.from_pretrained(\"gpt2\")\ntokenizer = tfms.AutoTokenizer.from_pretrained(\"gpt2\")\n\n# Generate text\ninput_text = \"Once upon a time\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutputs = model.generate(input_ids, max_length=50)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Code example","metadata":{}},{"cell_type":"markdown","source":"Here's an example of how to use the Transformers library with a built-in Python dataset, specifically the IMDb movie reviews dataset, to perform sentiment classification using a pre-trained BERT model:","metadata":{}},{"cell_type":"code","source":"import transformers as tfms\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import random_split\nfrom torchvision.datasets import IMDB\n\n# Load IMDb movie reviews dataset\ndataset = IMDB(root=\"./data\", split=\"train\")\n\n# Split dataset into training and validation sets\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Load pre-trained BERT tokenizer\ntokenizer = tfms.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Define custom dataset for BERT input encoding\nclass BertDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset, tokenizer):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n\n    def __getitem__(self, idx):\n        text, label = self.dataset[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=256,\n            return_tensors=\"pt\"\n        )\n        input_ids = encoding[\"input_ids\"].squeeze()\n        attention_mask = encoding[\"attention_mask\"].squeeze()\n        return input_ids, attention_mask, label\n\n    def __len__(self):\n        return len(self.dataset)\n\n# Create instances of the custom dataset\ntrain_bert_dataset = BertDataset(train_dataset, tokenizer)\nval_bert_dataset = BertDataset(val_dataset, tokenizer)\n\n# Define dataloaders\ntrain_dataloader = DataLoader(train_bert_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_bert_dataset, batch_size=32)\n\n# Load pre-trained BERT model for sequence classification\nmodel = tfms.BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Training loop\nfor epoch in range(5):\n    model.train()\n    for input_ids, attention_mask, labels in train_dataloader:\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for input_ids, attention_mask, labels in val_dataloader:\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            val_loss += outputs.loss.item()\n            _, predicted = torch.max(outputs.logits, dim=1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_loss /= len(val_dataloader)\n    accuracy = correct / total\n    print(f\"Epoch {epoch + 1}: Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-26T04:06:23.514214Z","iopub.execute_input":"2023-06-26T04:06:23.514558Z","iopub.status.idle":"2023-06-26T04:06:23.877967Z","shell.execute_reply.started":"2023-06-26T04:06:23.514530Z","shell.execute_reply":"2023-06-26T04:06:23.876225Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random_split\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMDB\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load IMDb movie reviews dataset\u001b[39;00m\n\u001b[1;32m      8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m IMDB(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'IMDB' from 'torchvision.datasets' (/opt/conda/lib/python3.10/site-packages/torchvision/datasets/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'IMDB' from 'torchvision.datasets' (/opt/conda/lib/python3.10/site-packages/torchvision/datasets/__init__.py)","output_type":"error"}]},{"cell_type":"markdown","source":"In this code, we load the IMDb movie reviews dataset, split it into training and validation sets, and create a custom dataset that encodes the text using the BERT tokenizer. We then define dataloaders for batch processing and load a pre-trained BERT model for sequence classification. The model is trained and evaluated using the training and validation datasets, and the process is repeated for multiple epochs.\n\nNote that you may need to install the required libraries and their dependencies using pip, such as transformers,","metadata":{}},{"cell_type":"markdown","source":"## Graphs","metadata":{}},{"cell_type":"markdown","source":"To graphically explain the Transformer architecture, we can utilize a visualization library such as matplotlib. Here's an example code snippet that provides a graphical representation of the Transformer model using a built-in Python dataset:","metadata":{}},{"cell_type":"code","source":"import transformers as tfms\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn\n\n# Load pre-trained BERT model\nmodel = tfms.BertModel.from_pretrained(\"bert-base-uncased\")\n\n# Define a sample input\ninput_text = \"Hello, how are you today?\"\ninput_ids = torch.tensor(model.tokenizer.encode(input_text)).unsqueeze(0)\n\n# Forward pass through the Transformer layers\noutputs = model.encoder(input_ids)\n\n# Get the attention weights\nattention_weights = outputs[-1]\n\n# Plot the attention weights\nnum_layers = len(attention_weights)\nnum_heads = attention_weights[0].shape[1]\nseq_length = attention_weights[0].shape[-1]\n\n# Create a grid of subplots\nfig, axes = plt.subplots(num_layers, num_heads, figsize=(10, 10))\n\n# Plot the attention weights for each layer and head\nfor layer in range(num_layers):\n    for head in range(num_heads):\n        ax = axes[layer, head]\n        ax.matshow(attention_weights[layer][0, head].detach().numpy(), cmap=\"viridis\")\n        ax.set_xticks(range(seq_length))\n        ax.set_yticks(range(seq_length))\n        ax.xaxis.set_label_position('top')\n        ax.xaxis.set_ticks_position('top')\n        ax.set_xlabel(\"Head {}\".format(head+1))\n        ax.set_ylabel(\"Layer {}\".format(layer+1))\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-26T04:06:01.654199Z","iopub.execute_input":"2023-06-26T04:06:01.654942Z","iopub.status.idle":"2023-06-26T04:06:19.952318Z","shell.execute_reply.started":"2023-06-26T04:06:01.654917Z","shell.execute_reply":"2023-06-26T04:06:19.950239Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"325880d6b7d2483d91bf02f11f913eff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"447a621dbed1430092178a95682db9fe"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Define a sample input\u001b[39;00m\n\u001b[1;32m     10\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, how are you today?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mencode(input_text))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass through the Transformer layers\u001b[39;00m\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder(input_ids)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n","\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'tokenizer'"],"ename":"AttributeError","evalue":"'BertModel' object has no attribute 'tokenizer'","output_type":"error"}]},{"cell_type":"markdown","source":"In this code, we load a pre-trained BERT model and provide a sample input text. We then perform a forward pass through the Transformer layers and extract the attention weights. Finally, we plot the attention weights for each layer and head using subplots, resulting in a visual representation of the Transformer's attention mechanism.\n\nNote that you may need to install the required libraries, including transformers, torch, matplotlib, and seaborn, using pip before running the code.","metadata":{}},{"cell_type":"markdown","source":"## Uses ","metadata":{}},{"cell_type":"markdown","source":"1. Natural Language Processing (NLP):\n\n* Sentiment Analysis: Transformers can analyze text to determine the sentiment (positive, negative, neutral) expressed in it.\n* Machine Translation: Transformers have been used to build powerful machine translation systems that can translate text from one language to another.\n* Named Entity Recognition: Transformers can identify and extract entities such as names, organizations, and locations from text.\n* Text Summarization: Transformers can generate concise summaries of long texts by capturing the most important information.\n\n2. Question Answering:\n\n* Transformers can understand and answer questions based on a given context, as demonstrated in systems like OpenAI's GPT models.\n\n3. Speech Recognition:\n\n*  Transformers have been used to build speech recognition systems that convert spoken language into written text.\n\n4. Image Classification and Generation:\n\n* Transformers can be applied to image classification tasks by treating images as sequences of patches or by combining them with convolutional neural networks (CNNs).\n* Transformers have also been used for image generation tasks, such as generating captions or completing missing parts of images.\n\n5. Recommender Systems:\n\n* Transformers have been utilized to build recommendation models that provide personalized recommendations to users based on their preferences and behavior.\n\n6. Time Series Analysis:\n\n*  Transformers can analyze and predict patterns in time series data, such as stock prices, weather data, or energy consumption.\n\n7. Reinforcement Learning:\n\n* Transformers have been combined with reinforcement learning algorithms to build intelligent agents that can learn to perform complex tasks in dynamic environments.\n\n8. Music Generation:\n\n* Transformers can generate new pieces of music by modeling patterns in musical sequences.","metadata":{}},{"cell_type":"markdown","source":"## ----------------------Project-----------------------------","metadata":{}},{"cell_type":"markdown","source":"[Project video](https://www.youtube.com/watch?v=kCc8FmEb1nY&pp=ygUrbWFjaGluZSBsZWFybmluZyB0cmFuc2Zvcm1lciBweWh0b24gcHJvamVjdA%3D%3D)\n\n","metadata":{}}]}